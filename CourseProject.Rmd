---
output:
  pdf_document: default
  html_document: default
---
# Course project 
## Practical Machine Learning

## Abstract


Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, the goal is: using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, predict the manner in which they did the exercises. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways: A - the correct way and B, C, D e E, four different wrong ways of do the exercise. This is the “classe” variable in the training set. It will be select any of the other variables to predict with.

More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.
The training and test data for this project are available in this two url’s:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

### Data Processing


```{r}
library(corrplot)
library(caret)
train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")
```

### Exploratory data analysis

What we see is a lot of data with NA / empty values. Let's remove those.


```{r}
Percentage_max_NA = 90
maxNACount <- nrow(train) / 100 * Percentage_max_NA
removeColumns <- which(colSums(is.na(train) | train=="") > maxNACount)
training.cleaned <- train[,-removeColumns]
testing.cleaned <- test[,-removeColumns]
```


```{r}
#that reduces the columns to only 60 columns
dim(training.cleaned)
dim(testing.cleaned)
```


```{r}
#Investigating the data we can see that the seven first columns have a sequencial number (the first)
#and variations of the timestamp that we are not using for this analysis so we will eliminate those columns remaining 53
trainOK<-training.cleaned[,-c(1:6)]
testOK<-testing.cleaned[,-c(1:6)]
dim(trainOK);dim(testOK)
```


```{r}
exerCorrmatrix<-cor(trainOK[sapply(trainOK, is.numeric)])  
corrplot(exerCorrmatrix,order="original", method="circle", type="lower", tl.cex=0.45, tl.col="black", number.cex=0.25)  
```
Since the test set provided is the the ultimate validation set, we will split the current training in a test and train set to work with.

```{r}
set.seed(2022)
inTrain<-createDataPartition(trainOK$classe, p=3/4, list=FALSE)
train<-trainOK[inTrain,]
valid<-trainOK[-inTrain,] 
```

Analysing the principal components, we got that 25 components are necessary to capture .95 of the variance. But it demands alot of machine processing so, we decided by a .80 thresh to capture 80% of the variance using 13 components


```{r}
PropPCA<-preProcess(train[,-54],method="pca", thresh=0.8)
PropPCA
```




### Preprocessing

```{r}
#create the preProc object, excluding the response (classe)
preProc  <- preProcess(train[,-54], 
                       method = "pca",
                       pcaComp = 13, thresh=0.8) 
#Apply the processing to the train and test data, and add the response 
#to the dataframes
train_pca <- predict(preProc, train[,-54])
train_pca$classe <- train$classe
#train_pca has only 13 principal components plus classe
valid_pca <- predict(preProc, valid[,-54])
valid_pca$classe <- valid$classe
#valid_pca has only 13 principal components plus classe
```


### Model examination

We will use the Random Forest model

```{r}
start <- proc.time()
fitControl<-trainControl(method="cv", number=5, allowParallel=TRUE)
fit_rf<-train(classe ~., data=train_pca, method="rf", trControl=fitControl)
print(fit_rf, digits=4) 
proc.time() - start
```



```{r}
predict_rf<-predict(fit_rf,valid_pca)  
(conf_rf<-confusionMatrix(as.factor(valid_pca$classe), predict_rf))
(accuracy_rf<-conf_rf$overall['Accuracy'])
```

We can now say that for this dataset, random forest method has an accuracy of 0.96


###Prediction on Testing Set

Applying the Random Forest to predict the outcome variable classe for the test set.

```{r}
test_pca <- predict(preProc, testOK[,-54])
test_pca$problem_id <- testOK$problem_id
(predict(fit_rf, test_pca))
```

with those 20 predictions we conclude the Course Project






